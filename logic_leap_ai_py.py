# -*- coding: utf-8 -*-
"""Logic Leap Ai.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ARTiBhw3j1O8ynxF7yipADhFjF4htQUl

1 : Install dependencies (Colab cell)
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q pandas numpy scikit-learn statsmodels xgboost joblib typer fastapi uvicorn

""" 2 : Upload dataset

"""

from google.colab import files
import pandas as pd

uploaded = files.upload()

filename = list(uploaded.keys())[0]

df = pd.read_csv(filename)
df.head()

"""3 : Create folders + imports + reproducibility

"""

import os, math, json
from pathlib import Path
import pandas as pd, numpy as np
from sklearn.metrics import mean_absolute_error
import joblib
import warnings
warnings.filterwarnings("ignore")

SEED = 42
np.random.seed(SEED)

os.makedirs('outputs', exist_ok=True)
os.makedirs('models', exist_ok=True)
DATA_DIR = Path("data")

from google.colab import drive
drive.mount('/content/drive')

"""4 : Loader + quick sanity checks"""

def load_data(data_dir="."):
    ops_path = os.path.join(data_dir, "operations_daily_365d.csv")
    meta_path = os.path.join(data_dir, "site_meta.csv")
    assert os.path.exists(ops_path), f"Missing {ops_path}. Please ensure operations_daily_365d.csv is in the current directory."
    ops = pd.read_csv(ops_path, parse_dates=["date"])
    meta = None
    if os.path.exists(meta_path):
        meta = pd.read_csv(meta_path)
    return ops, meta

ops, meta = load_data()
print("ops rows:", len(ops))
print("columns:", ops.columns.tolist())
print("date range:", ops['date'].min(), "→", ops['date'].max())
print("unique sites:", ops['site_id'].nunique())
display(ops.head())

"""5 : Preprocessing & feature engineering"""

def preprocess_basic(df):
    df = df.copy()
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values(['site_id','date']).reset_index(drop=True)
    for c in ['units_produced','power_kwh','runtime_hours','downtime_hours']:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors='coerce')
            df[c] = df[c].fillna(0.0)
    return df

def create_date_feats(df):
    df = df.copy()
    df['day_of_week'] = df['date'].dt.dayofweek
    df['is_weekend'] = df['day_of_week'].isin([5,6]).astype(int)
    df['day'] = df['date'].dt.day
    df['month'] = df['date'].dt.month
    return df

def add_lags(df, col, lags=(1,7,14), rwindows=(7,14)):
    df = df.copy()
    df = df.sort_values(['site_id','date'])
    for lag in lags:
        df[f'{col}_lag_{lag}'] = df.groupby('site_id')[col].shift(lag)
    for w in rwindows:
        df[f'{col}_rwmean_{w}'] = df.groupby('site_id')[col].shift(1).rolling(window=w, min_periods=1).mean()
    return df

df = preprocess_basic(ops)
df = create_date_feats(df)
df = add_lags(df, 'units_produced', lags=(1,7,14), rwindows=(7,14))
df = add_lags(df, 'power_kwh', lags=(1,7,14), rwindows=(7,14))

df_trainable = df.dropna(subset=['units_produced_lag_1','power_kwh_lag_1']).copy()
print("Total rows after lagging:", len(df_trainable))
df_trainable.head(3)

"""6 : Baseline forecast (7-day mean) and evaluation"""

def safe_mape(y_true, y_pred):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    denom = np.where(np.abs(y_true) < 1e-8, 1.0, np.abs(y_true))
    return np.mean(np.abs((y_true - y_pred) / denom)) * 100

last_date = df_trainable['date'].max()
cutoff = last_date - pd.Timedelta(days=28)
train_df = df_trainable[df_trainable['date'] <= cutoff]
test_df = df_trainable[df_trainable['date'] > cutoff]

print("Train rows:", len(train_df), "Test rows:", len(test_df))

baseline_preds, baseline_trues = [], []
for site, g in test_df.groupby('site_id'):
    hist = train_df[train_df['site_id']==site].sort_values('date')
    if len(hist)==0:
        continue
    last7 = hist['units_produced'].tail(7)
    if len(last7)==0:
        mean7 = hist['units_produced'].mean()
    else:
        mean7 = last7.mean()
    preds = [mean7] * len(g)
    baseline_preds.extend(preds)
    baseline_trues.extend(g['units_produced'].tolist())

baseline_mae = mean_absolute_error(baseline_trues, baseline_preds)
baseline_mape = safe_mape(baseline_trues, baseline_preds)
print(f"Baseline (units_produced) — MAE: {baseline_mae:.3f}, MAPE: {baseline_mape:.2f}%")

"""7 : Train improved model (global XGBoost) for both targets"""

import xgboost as xgb
from sklearn.model_selection import train_test_split

df_trainable['site_code'] = df_trainable['site_id'].astype('category').cat.codes

last_date = df_trainable['date'].max()
cutoff = last_date - pd.Timedelta(days=28)
train_df = df_trainable[df_trainable['date'] <= cutoff]
test_df = df_trainable[df_trainable['date'] > cutoff]

print("Train rows:", len(train_df), "Test rows:", len(test_df))

feat_candidates = [c for c in df_trainable.columns if
                   (c.startswith('units_produced_lag_') or c.startswith('units_produced_rwmean_') or
                    c.startswith('power_kwh_lag_') or c.startswith('power_kwh_rwmean_'))]
feat_candidates += ['day_of_week','is_weekend','site_code']
features = [f for f in feat_candidates if f in df_trainable.columns]
print("Using features:", features)

X_train = train_df[features].fillna(0)

y_train_u = train_df['units_produced']
model_units = xgb.XGBRegressor(objective='reg:squarederror', random_state=SEED, n_estimators=200, verbosity=0)
model_units.fit(X_train, y_train_u)

y_train_p = train_df['power_kwh']
model_power = xgb.XGBRegressor(objective='reg:squarederror', random_state=SEED, n_estimators=200, verbosity=0)
model_power.fit(X_train, y_train_p)

joblib.dump(model_units, "models/xgb_units.joblib")
joblib.dump(model_power, "models/xgb_power.joblib")
print("Models trained and saved to /models")

"""8 : Evaluate improved model (simulate recursive forecasts for test period)"""

def recursive_forecast_for_period(site_df, model_u, model_p, features, horizon):
    """
    site_df: dataframe for this site containing chronological history up to last real date before forecast
    horizon: number of days to forecast
    """
    site_df = site_df.sort_values('date').copy().reset_index(drop=True)
    hist_units = list(site_df['units_produced'].tolist())
    hist_power = list(site_df['power_kwh'].tolist())
    last_date = site_df['date'].max()
    preds_u, preds_p = [], []
    for h in range(1, horizon+1):
        next_date = last_date + pd.Timedelta(days=h)
        def get_lag(arr, lag):
            if len(arr) >= lag:
                return arr[-lag]
            else:
                return arr[0] if len(arr)>0 else 0.0
        feat = {}
        for lag in (1,7,14):
            feat[f'units_produced_lag_{lag}'] = get_lag(hist_units, lag)
            feat[f'power_kwh_lag_{lag}'] = get_lag(hist_power, lag)
        for w in (7,14):
            feat[f'units_produced_rwmean_{w}'] = np.mean(hist_units[-w:]) if len(hist_units)>0 else 0.0
            feat[f'power_kwh_rwmean_{w}'] = np.mean(hist_power[-w:]) if len(hist_power)>0 else 0.0
        feat['day_of_week'] = next_date.weekday()
        feat['is_weekend'] = 1 if next_date.weekday() in (5,6) else 0
        feat['site_code'] = int(site_df['site_code'].iat[-1]) if 'site_code' in site_df.columns else 0
        Xrow = pd.DataFrame([feat])[features].fillna(0)
        pu = model_u.predict(Xrow)[0]
        pp = model_p.predict(Xrow)[0]
        preds_u.append(float(pu)); preds_p.append(float(pp))
        hist_units.append(pu); hist_power.append(pp)
    return preds_u, preds_p

improved_trues, improved_preds = [], []
for site, gtest in test_df.groupby('site_id'):
    hist = df_trainable[(df_trainable['site_id']==site) & (df_trainable['date'] <= cutoff)]
    if len(hist) == 0:
        continue
    horizon = len(gtest)
    pu, pp = recursive_forecast_for_period(hist, model_units, model_power, features, horizon)
    improved_preds.extend(pu)
    improved_trues.extend(gtest['units_produced'].tolist())

improved_mae = mean_absolute_error(improved_trues, improved_preds)
improved_mape = safe_mape(improved_trues, improved_preds)
print(f"Improved (units_produced) — MAE: {improved_mae:.3f}, MAPE: {improved_mape:.2f}%")
print("Delta vs baseline: MAE improvement:", baseline_mae - improved_mae, "MAPE improvement:", baseline_mape - improved_mape)

"""9 : Produce 14-day forecasts for every site and save CSVs"""

HORIZON = 14
forecast_rows_units = []
forecast_rows_power = []
sites = sorted(df_trainable['site_id'].unique())

site_code_map = df_trainable[['site_id','site_code']].drop_duplicates().set_index('site_id')['site_code'].to_dict()
for site in sites:
    hist = df_trainable[df_trainable['site_id']==site].sort_values('date').copy()
    if len(hist) == 0:
        continue
    hist['site_code'] = site_code_map.get(site, 0)
    p_u, p_p = recursive_forecast_for_period(hist, model_units, model_power, features, HORIZON)
    last_date = hist['date'].max()
    for i in range(HORIZON):
        forecast_rows_units.append({
            'site_id': site,
            'date': (last_date + pd.Timedelta(days=i+1)).strftime("%Y-%m-%d"),
            'metric': 'units_produced',
            'prediction': float(p_u[i])
        })
        forecast_rows_power.append({
            'site_id': site,
            'date': (last_date + pd.Timedelta(days=i+1)).strftime("%Y-%m-%d"),
            'metric': 'power_kwh',
            'prediction': float(p_p[i])
        })

df_forecast_units = pd.DataFrame(forecast_rows_units)
df_forecast_power = pd.DataFrame(forecast_rows_power)
df_forecast_units.to_csv('outputs/forecast_units.csv', index=False)
df_forecast_power.to_csv('outputs/forecast_power.csv', index=False)
print("Saved outputs/forecast_units.csv and outputs/forecast_power.csv")

"""10 : Downtime anomaly detection → alerts.csv"""

from scipy import stats

def detect_downtime_alerts(df):
    d = df.copy()
    d['power_zero'] = (d['power_kwh'] <= 1e-3).astype(int)
    r1 = d[(d['power_zero']==1) & (d['units_produced'] <= 1e-3)].copy()
    r1['anomaly_type'] = 'power_zero_downtime'
    r1['score'] = 1.0

    d['rw7_units'] = d.groupby('site_id')['units_produced'].transform(lambda s: s.rolling(7, min_periods=1).mean())
    d['resid'] = d['units_produced'] - d['rw7_units']
    d['z'] = d.groupby('site_id')['resid'].transform(lambda x: stats.zscore(x.fillna(0)))
    r2 = d[d['z'] < -2.5].copy()
    r2['anomaly_type'] = 'sudden_drop_units'
    r2['score'] = (-r2['z']) / max(5.0, r2['z'].abs().max())

    anomalies = pd.concat([r1, r2], axis=0).drop_duplicates(subset=['site_id','date','anomaly_type'])
    if anomalies.empty:
        return pd.DataFrame(columns=['site_id','date','metric','score_raw','severity','units_produced','power_kwh'])
    out = anomalies[['site_id','date','anomaly_type','score','units_produced','power_kwh']].copy()
    out = out.rename(columns={'anomaly_type':'metric','score':'score_raw'})
    out['severity'] = out['score_raw'].apply(lambda s: 'HIGH' if s>0.8 else ('MEDIUM' if s>0.4 else 'LOW'))
    out = out.sort_values(['site_id','date'])
    out['date'] = out['date'].dt.strftime("%Y-%m-%d")
    return out

alerts = detect_downtime_alerts(df)
alerts.to_csv('outputs/alerts.csv', index=False)
print("Saved outputs/alerts.csv — example rows (top 10):")
print(alerts.head(10))

"""11 : Download outputs"""

from google.colab import files
files.download('outputs/forecast_units.csv')
files.download('outputs/forecast_power.csv')
files.download('outputs/alerts.csv')

"""12 : write minimal FastAPI app files"""

app_code = r'''
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
app = FastAPI(title="Forecast+Alerts API")
FU = pd.read_csv("outputs/forecast_units.csv", parse_dates=["date"])
FP = pd.read_csv("outputs/forecast_power.csv", parse_dates=["date"])
AL = pd.read_csv("outputs/alerts.csv", parse_dates=["date"]) if os.path.exists("outputs/alerts.csv") else pd.DataFrame()

class Q(BaseModel):
    site_id: str
    start_date: str = None
    end_date: str = None

def filter_df(df, site_id, start_date, end_date):
    out = df[df['site_id'] == site_id]
    if start_date:
        out = out[out['date'] >= start_date]
    if end_date:
        out = out[out['date'] <= end_date]
    return out

@app.post("/forecast_units")
def forecast_units(q: Q):
    df = filter_df(FU, q.site_id, q.start_date, q.end_date)
    if df.empty:
        raise HTTPException(404, "No forecasts")
    return df.to_dict(orient='records')

@app.post("/alerts")
def alerts(q: Q):
    df = filter_df(AL, q.site_id, q.start_date, q.end_date)
    return df.to_dict(orient='records')
'''
open('app_main.py','w').write(app_code)
print("Wrote app_main.py. To run locally:")
print("  pip install fastapi uvicorn")
print("  uvicorn app_main:app --host 0.0.0.0 --port 8000")

"""13 : One-page executive brief"""

from datetime import date
brief = f"""
Executive Brief — Forecasting & Alerting Pipeline
Prepared: {date.today().isoformat()}

What was built
- Data pipeline: preprocessing, date features, lag & rolling features.
- Baseline: 7-day mean forecast (MAE/MAPE computed).
- Improved: global XGBoost models for units_produced and power_kwh (trained on lag features).
- Downtime alerts: rule-based (zero power+zero units) and statistical drop detection (z-score).
- Outputs: outputs/forecast_units.csv, outputs/forecast_power.csv, outputs/alerts.csv
- API: simple FastAPI app (app_main.py) to serve forecasts & alerts.

Key metrics (units_produced):
- Baseline MAE / MAPE: {baseline_mae:.3f} / {baseline_mape:.2f}%
- Improved MAE / MAPE: {improved_mae:.3f} / {improved_mape:.2f}%

Automation triggers
- HIGH severity → immediate pager/webhook and remote diagnostics.
- Retrain model daily or when aggregated MAPE > 15% (auto-trigger).
- Save outputs to S3 or shared store and call API.

Next steps
- Add per-site models for sites with distinct behavior.
- Add external regressors (weather, shift schedules).
- Add CI, model versioning, and scheduling (cron / Airflow).

"""

open('outputs/executive_brief.txt','w').write(brief)
print("Saved outputs/executive_brief.txt")
print(brief)